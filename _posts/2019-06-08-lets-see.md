---
layout: post
title: "lets see"
date: 2019-06-08
mathjax: True
---

In my previous two blogs, you see how vae helps to create <a href="https://shiva1515.github.io/2019/06/05/All-you-need-to-know-about-Vae-(Part-2).html#SMILES" target="_blank">SMILES</a>(text generation) of a similar kind.<br/>

there are many models which work similarly to the VAE.they also helps us to understand the similar input generation. But what's the difference between VAE and others it they both work on the same things.
Why we use other models instead of VAE for similar input generation. Is there any drawbacks of VAE over other models. <br/>

Today we will see about drawbacks of VAE and how Generative adversarial network and Adversarial autoencoder is better than Variational autoencoder.
The assumption we took in VAE is that we used another distribution which is Gaussian distribution and imposed this distribution to our latent vector distribution because we don’t know the distribution of input data
and to do this we use KL-divergence to make the similar distribution.<br/>

To know more about VAE and KL-divergence please refer to my previous <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html" target="_blank">blogs</a><br/>
we take assumption when using KL-divergence that the two distribution will overlap each other.but what if they don’t overlap each other.
if this not happen then our KL-divergence gives the $\infty$ value which gives us some wired results and non trackable. lets understand this by an simple example
lets say $Q(x)$ and $P(x)$ be the probability distribution function and we want to measure the KL-divergence of these distributions.<br/>

And we have values of 
$$Q(x=0) =0$$ and $$P(x=1) = 1$$ 
$KL(P||Q)_{x=0}  = \sum_x P(x)log(P(x)/Q(x))$
$KL(P||Q)_{x=0} = \infinity$

To overcome this drawback, we use Jensen-Shannon Divergence(JSD) divergence and Wasserstein divergence.

JSD divergence: It is a method to measure the similarity between two probability distribution function. 
It is the same as the KL-divergence method but with better results.<br/>
JSD-divergence = $JSD(P||Q) = ½[KL(P||M) + KL(Q||M)]$<br/>
Where $M = (P + Q)/2$<br/>

advantange of JSD over KL-divergence is its symmetric nature means $JSD(P||Q) = JSD(Q||P)$<br/>
now lets again see the above example we measure the JSD divergence between two probability function $P(x)$ and $Q(x)$ <br/>
$JSD(P||Q)_{x=0} = ½[P(x=0)log(P(x=0)/(P(x) + Q(x))/2)  + Q(x=0)log(Q(x=0)/(Q(x) + P(x))/2)$<br/>
$JSD(P||Q)_{x=0} = log2$<br/>

And the advantage of GAN over VAE is it uses JSD divergence instead of KL-divergence.<br/>
Now there is one problem in JSD divergence, and  this is when the two distribution function far from each other then it gives constant value log2 and when we take the gradient of JSD divergence after some time it provides 0 value.<br/>

this is not better for the model. So now we want another method which helps us to measure gradient as well as at the same time it also helps to measure the similarity between two probability distribution function.<br/>

To overcome this, we use Wasserstein distance, which is also called earth mover distance.
I am not going deep in this concept because then blog will we too large. In reference, I will provide you with a link if we want to know the concepts of Wasserstein distance.
When we use Wasserstein distance in GAN, we called GAN as WGAN.<br/>

*How AAE is used*
An adversarial autoencoder is a type Generative adversarial network in which we have an autoencoder and a discriminator.<br/>
In Autoencoder part we have <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html#encoder" target="_blank">Encoder</a>, <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html#Decoder" target="_blank">Decoder</a> and <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html#latent vector" target="_blank">Latent vector</a> vector. Please click on the respective link if you want to know about these terms.<br/>
auto-encoder try to give output as same as the input, but we want to generate a similar input, not same input so what we do is we take a sample from latent vector and put into the decoder to give similar output, but the problem is we don’t know the distribution of latent vector.<br/>
So we take a random distribution whose distribution we know and try to impose this distribution into latent distribution and remember to impose this distribution into the latent distribution we use JSD divergence because we see JSD is better than KL-divergence.<br/>

{% include image.html url="/assets/img/autoencoder.png" description="" %}

You can see in above image that x is our real distribution and $q(z|x)$ is our encoder, $z\sim q(z)$ is a sample taken from $q(z)$, and from this distribution, we make a similar distribution.<br/>

**Theory of AAE**

To make adversarial autoencoder, we first make train our autoencoder to make the same images. Why this we do earlier you will understand later, after the autoencoder part  trained now we trained our generator part now what we do is first we take sample from our dataset we pass it through encoder now we take sample and give it to the discriminator and make them labels as 0.and we took sample from real data p(z) and pass it to the discriminator and make them labels as1.<br/>

Remember one thing here discriminator act as classifier it only classifies the data which is coming from the dataset and real distribution, and the discriminator work is to differentiate this. And we trained the discriminator until the discriminator not able to differentiate between the real dataset and fake and when this happens we understand that now our $q(z)$ is somewhat similar to the $p(z)$ and now we take a sample and pass through the decoder to make similar input.<br/>

**Loss Function of AAE**
Reconstruction Loss: this loss try to minimize the error between real image and generated image<br/>
Regularization Loss: $min_E max_D {E_{z \smi p_z(z)}[logD(z)] + E_{x \smi p_d(x)}[log(1-D(E(x))]}$.<br/>
To understand the regularization Loss first understand the terms <br/>
$D(z)$ is sample taken from distribution of real sample means taken from p(z).<br/>
$p_d(x)$ is taken data from dataset.<br/>
$D(E(x))$ is data coming from encoder and then feed into the discriminator.<br/>

To understand why in regularization loss minmax function is there please visit <a href="https://www.youtube.com/watch?v=ZD7HtL1gook&list=PLdxQ7SoCLQAMGgQAIAcyRevM8VvygTpCu&index=2" target="_blank">Loss</a> this video explains the concept easily.<br/>

*Note*<br/>
Here encoder plays two crucial roles 
<ol>
  <li>encoder when we use an autoencoder</li>
  <li>generator when we use GAN training means when we use discriminator.</li>
</ol>

Let's see the code of AAE in pytorch.in this code, we see text generation by using SMILES(molecular generation)

I used pycharm; I suggest you use google collab for this code and please reduce the size of the data as real data needs a huge computation.

**Import dataset and data-preprocessing:**
We use the same <a href="https://shiva1515.github.io/2019/06/05/All-you-need-to-know-about-Vae-(Part-2).html#Import_dataset" target="_blank">dataset</a> as we use in VAE to see the <a href="https://shiva1515.github.io/2019/06/05/All-you-need-to-know-about-Vae-(Part-2).html#Build vocabulary" target="_blank">dvocabulary</a><br/>
The data preprocessing part is same in VAE and AAE. and i explain about data preprocessing also.

**Model:**
In the model section, we make 3 class that I already told you encoder, decoder and discriminator.
We feed our vocabulary first to encoder this encoder tries to encode the input data and make a small vector which representation our whole input data now the decoder takes input from this small dimension latent vector and convert back into our original data.

```python
import torch

import pandas as pd
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn


from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence


from data import *


emb_dim = 30
hidden_dim = 64
latent_dim = 4
disc_input = 64
disc_output = 84
batch_size = 50


class encoder(nn.Module):
    def __init__(self, vocab, emb_dim, hidden_dim, latent_dim):
        super(encoder, self).__init__()
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.emb_dim = emb_dim
        self.vocab = vocab

        self.embeddings_layer = nn.Embedding(len(vocab), emb_dim, padding_idx=c2i['<pad>'])

        self.rnn = nn.LSTM(emb_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, latent_dim)
        self.relu = nn.ReLU()
        nn.Drop = nn.Dropout(p=0.25)

    def forward(self, x, lengths):
        batch_size = x.shape[0]

        x = self.embeddings_layer(x)
        x = pack_padded_sequence(x, lengths, batch_first=True)
        output, (_, x) = self.rnn(x)

        x = x.permute(1, 2, 0).view(batch_size, -1)
        x = self.fc(x)
        state = self.relu(x)
        return state


class decoder(nn.Module):
    def __init__(self, vocab, emb_dim, latent_dim, hidden_dim):
        super(decoder, self).__init__()
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim
        self.emb_dim = emb_dim
        self.vocab = vocab

        self.latent = nn.Linear(latent_dim, hidden_dim)
        self.embeddings_layer = nn.Embedding(len(vocab), emb_dim, padding_idx=c2i['<pad>'])
        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, len(vocab))

    def forward(self, x, lengths, state, is_latent_state=False):
        if is_latent_state:
            c0 = self.latent(state)

            c0 = c0.unsqueeze(0)
            h0 = torch.zeros_like(c0)

            state = (h0, c0)

        x = self.embeddings_layer(x)

        x = pack_padded_sequence(x, lengths, batch_first=True)

        x, state = self.rnn(x, state)

        x, lengths = pad_packed_sequence(x, batch_first=True)
        x = self.fc(x)

        return x, lengths, state


class Discriminator(nn.Module):
    def __init__(self, latent_dim, disc_input, disc_output):
        super(Discriminator, self).__init__()
        self.latent_dim = latent_dim
        self.disc_input = disc_input
        self.disc_output = disc_output

        self.lin1 = nn.Linear(latent_dim, disc_input)
        self.lin2 = nn.Linear(disc_input, disc_output)
        self.lin3 = nn.Linear(disc_output, 1)
        self.sig = nn.Sigmoid()

    def forward(self, x):
        x = self.lin1(x)
        x = self.lin2(x)
        x = self.lin3(x)

        x = self.sig(x)
        return x
class AAE(nn.Module):
    def __init__(self):
        super(AAE,self).__init__()
        self.encoder = encoder(vocab,emb_dim,hidden_dim,latent_dim)
        self.decoder = decoder(vocab,emb_dim,latent_dim,hidden_dim)
        self.discriminator = Discriminator(latent_dim,disc_input,disc_output)
        
```
**Training:**
To train the model first, we have to divide the model into two parts first is autoencoder which is encoder and decoder by doing this we can easily regenerate our data back this training we call as pre-train.<br/>
After pretraining, we train our generative part so now we use encoder and discriminator in this training when data comes from the real dataset, i.e. from encoder we make them label 0 and when data comes from real distribution we make them label 1. And feed into the discriminator, it tries to discriminate, but we train them until our discriminator does not differentiate between encoder input and real distribution input.<br/>

```python
from torch.utils.data import DataLoader

import torch.nn.functional as F
from model import *

def pretrain(model, train_loader):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=0.001)
    model.zero_grad()
    for epoch in range(4):
        if optimizer is None:
            model.train()
        else:
            model.eval()
        for i, (encoder_inputs, decoder_inputs, decoder_targets) in enumerate(train_loader):
            encoder_inputs = (data.to(device) for data in encoder_inputs)
            decoder_inputs = (data.to(device) for data in decoder_inputs)
            decoder_targets = (data.to(device) for data in decoder_targets)

            latent_code = model.encoder(*encoder_inputs)
            decoder_output, decoder_output_lengths, states = model.decoder(*decoder_inputs, latent_code,
                                                                           is_latent_state=True)

            decoder_outputs = torch.cat([t[:l] for t, l in zip(decoder_output, decoder_output_lengths)], dim=0)
            decoder_targets = torch.cat([t[:l] for t, l in zip(*decoder_targets)], dim=0)
            loss = criterion(decoder_outputs, decoder_targets)

            if optimizer is not None:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()


def train(model, train_loader):
    criterion = {"enc": nn.CrossEntropyLoss(), "gen": lambda t: -torch.mean(F.logsigmoid(t)),"disc": nn.BCEWithLogitsLoss()}

    optimizers = {'auto': torch.optim.Adam(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=0.001),
        'gen': torch.optim.Adam(model.encoder.parameters(), lr=0.001),
        'disc': torch.optim.Adam(model.discriminator.parameters(), lr=0.001)}

    model.zero_grad()
    for epoch in range(10):
        if optimizers is None:
            model.train()
        else:
            model.eval()

        for i, (encoder_inputs, decoder_inputs, decoder_targets) in enumerate(train_loader):
            encoder_inputs = (data.to(device) for data in encoder_inputs)
            decoder_inputs = (data.to(device) for data in decoder_inputs)
            decoder_targets = (data.to(device) for data in decoder_targets)

            latent_code = model.encoder(*encoder_inputs)
            decoder_output, decoder_output_lengths, states = model.decoder(*decoder_inputs, latent_code,
                                                                           is_latent_state=True)
            discriminator_output = model.discriminator(latent_code)

            decoder_outputs = torch.cat([t[:l] for t, l in zip(decoder_output, decoder_output_lengths)], dim=0)
            decoder_targets = torch.cat([t[:l] for t, l in zip(*decoder_targets)], dim=0)

            autoencoder_loss = criterion["enc"](decoder_outputs, decoder_targets)
            generation_loss = criterion["gen"](discriminator_output)

            if i % 2 == 0:
                discriminator_input = torch.randn(batch_size, latent_dim)
                discriminator_output = model.discriminator(discriminator_input)
                discriminator_targets = torch.ones(batch_size, 1)
            else:
                discriminator_targets = torch.zeros(batch_size, 1)
            discriminator_loss = criterion["disc"](discriminator_output, discriminator_targets)

            if optimizers is not None:
                optimizers["auto"].zero_grad()
                autoencoder_loss.backward(retain_graph=True)
                optimizers["auto"].step()

                optimizers["gen"].zero_grad()
                autoencoder_loss.backward(retain_graph=True)
                optimizers["gen"].step()

                optimizers["disc"].zero_grad()
                autoencoder_loss.backward(retain_graph=True)
                optimizers["disc"].step()

def fit(model,train_data):
    train_loader = get_dataloader(model, train_data, collate_fn=None, shuffle=True)
    pretrain(model,train_loader)
    train(model,train_loader)

def get_collate_device(model):
    return device
def get_dataloader(model, data, collate_fn=None, shuffle=True):
    if collate_fn is None:
        collate_fn = get_collate_fn(model)
    return DataLoader(data, batch_size= batch_size,shuffle=shuffle,collate_fn=collate_fn)


def get_collate_fn(model):
    device = get_collate_device(model)

    def collate(data):
        data.sort(key=lambda x: len(x), reverse=True)

        tensors = [string2tensor(string, device=device) for string in data]
        lengths = torch.tensor([len(t) for t in tensors], dtype=torch.long, device=device)

        encoder_inputs = pad_sequence(tensors, batch_first=True, padding_value=c2i["<pad>"])
        encoder_input_lengths = lengths - 2

        decoder_inputs = pad_sequence([t[:-1] for t in tensors], batch_first=True, padding_value=c2i["<pad>"])
        decoder_input_lengths = lengths - 1
        decoder_targets = pad_sequence([t[1:] for t in tensors], batch_first=True, padding_value=c2i["<pad>"])
        decoder_target_lengths = lengths - 1
        return (encoder_inputs, encoder_input_lengths), (decoder_inputs, decoder_input_lengths), (decoder_targets, decoder_target_lengths)

    return collate
```
**Sampling:**
After training is done, our model completes, now our real distribution well imposed on our latent distribution.<br/>
Now we take samples from our latent distribution and feed into the decoder generate the similar data as of input data.<br/>

```python
from model import *
from tqdm import tqdm
from data import *
def sample(model,n_batch, max_len=100):
    with torch.no_grad():
        samples = []
        lengths = torch.zeros(n_batch, dtype=torch.long, device=device)
        state = sample_latent(n_batch)
        prevs = torch.empty(n_batch, 1, dtype=torch.long, device=device).fill_(c2i["<bos>"])
        one_lens = torch.ones(n_batch, dtype=torch.long, device=device)
        is_end = torch.zeros(n_batch, dtype=torch.uint8, device=device)
        for i in range(max_len):
            logits, _, state = model.decoder(prevs, one_lens, state, i == 0)
            currents = torch.argmax(logits, dim=-1)
            is_end[currents.view(-1) == c2i["<eos>"]] = 1
            if is_end.sum() == max_len:
                break

            currents[is_end, :] = c2i["<pad>"]
            samples.append(currents)
            lengths[~is_end] += 1
            prevs = currents
    if len(samples):
        samples = torch.cat(samples, dim=-1)
        samples = [tensor2string(t[:l]) for t, l in zip(samples, lengths)]
    else:
        samples = ['' for _ in range(n_batch)]
    return samples
```











