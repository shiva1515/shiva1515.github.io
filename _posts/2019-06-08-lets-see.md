---
layout: post
title: "lets see"
date: 2019-06-08
mathjax: True
---

In my previous two blogs, you see how vae helps to create SMILES(text generation) of a similar kind.<br/>

there are many models which work similarly to the VAE.they also helps us to understand the similar input generation. But what's the difference between VAE and others it they both work on the same things.
Why we use other models instead of VAE for similar input generation. Is there any drawbacks of VAE over other models. <br/>

Today we will see about drawbacks of VAE and how Generative adversarial network and Adversarial autoencoder is better than Variational autoencoder.
The assumption we took in VAE is that we used another distribution which is Gaussian distribution and imposed this distribution to our latent vector distribution because we don’t know the distribution of input data
and to do this we use KL-divergence to make the similar distribution.<br/>

To know more about VAE and KL-divergence please refer to my previous <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html" target="_blank">blogs</a><br/>
we take assumption when using KL-divergence that the two distribution will overlap each other.but what if they don’t overlap each other.
if this not happen then our KL-divergence gives the $\infty$ value which gives us some wired results and non trackable. lets understand this by an simple example
lets say $Q(x)$ and $P(x)$ be the probability distribution function and we want to measure the KL-divergence of these distributions.<br/>

And we have values of $Q(x)_{x=0} = 0$ and $P(x)_{x=1} = 1$ then <br/>
$KL(P||Q)_{x=0}  = \sum_x P(x)log(P(x)/Q(x))$<br/>
$KL(P||Q)_{x=0} = 1log1/0$<br/>
$KL(P||Q)_{x=0} = \infinity$<br/>

To overcome this drawback, we use Jensen-Shannon Divergence(JSD) divergence and Wasserstein divergence.

JSD divergence: It is a method to measure the similarity between two probability distribution function. 
It is the same as the KL-divergence method but with better results.<br/>
JSD-divergence = $JSD(P||Q) = ½[KL(P||M) + KL(Q||M)]$<br/>
Where $M = (P + Q)/2$<br/>

advantange of JSD over KL-divergence is its symmetric nature means $JSD(P||Q) = JSD(Q||P)$<br/>
now lets again see the above example we measure the JSD divergence between two probability function $P(x)$ and $Q(x)$ <br/>
$JSD(P||Q)_{x=0} = ½[P(x=0)log(P(x=0)/(P(x) + Q(x))/2)  + Q(x=0)log(Q(x=0)/(Q(x) + P(x))/2)$<br/>
$JSD(P||Q)_{x=0} = log2$<br/>

And the advantage of GAN over VAE is it uses JSD divergence instead of KL-divergence.<br/>
Now there is one problem in JSD divergence, and  this is when the two distribution function far from each other then it gives constant value log2 and when we take the gradient of JSD divergence after some time it provides 0 value.<br/>

this is not better for the model. So now we want another method which helps us to measure gradient as well as at the same time it also helps to measure the similarity between two probability distribution function.<br/>

To overcome this, we use Wasserstein distance, which is also called earth mover distance.
I am not going deep in this concept because then blog will we too large. In reference, I will provide you with a link if we want to know the concepts of Wasserstein distance.
When we use Wasserstein distance in GAN, we called GAN as WGAN.<br/>

*How AAE is used*
An adversarial autoencoder is a type Generative adversarial network in which we have an autoencoder and a discriminator.<br/>
In Autoencoder part we have <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html#encoder" target="_blank">encoder</a>, <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html#Decoder" target="_blank">Dncoder</a> and <a href="https://shiva1515.github.io/2019/06/04/All-you-need-to-know-about-Vae-(Part-1).html#latent vector" target="_blank">latent vector</a> vector. Please click on the respective link if you want to know about these terms.<br/>
auto-encoder try to give output as same as the input, but we want to generate a similar input, not same input so what we do is we take a sample from latent vector and put into the decoder to give similar output, but the problem is we don’t know the distribution of latent vector.
So we take a random distribution whose distribution we know and try to impose this distribution into latent distribution and remember to impose this distribution into the latent distribution we use JSD divergence because we see JSD is better than KL-divergence.



